{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_excel(\"data/symp.xlsx\")\n",
    "data2 = data2.iloc[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Often feeling worrying too much': 'Often feeling worrying too much',\n",
       " 'hard to concentrate, need to drink everyday': 'hard to concentrate, need to drink everyday',\n",
       " 'anxiety': 'anxiety',\n",
       " 'Tremling, dizzy, frackle, soarthroat': 'Tremling, dizzy, frackle, soarthroat',\n",
       " 'Fainting': 'Fainting',\n",
       " 'mood swing, happy without reason, sad without reason': 'mood swing, happy without reason, sad without reason',\n",
       " 'Keringat dingin, dada berdegup kencang, panik': 'Cold sweat, chest throbbing, panic',\n",
       " 'extremely nervous, shaking, losing focus': 'extremely nervous, shaking, losing focus',\n",
       " 'I drink alcohol to often and I feel light pain': 'I drink alcohol to often and I feel light pain',\n",
       " 'Needing things orderly, fear of dirt, having difficulty tolerating uncertainty': 'Needing things orderly, fear of dirt, having difficulty tolerating uncertainty'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data2['q_11']\n",
    "unique_elements = data.unique()\n",
    "\n",
    "translations = {}\n",
    "for element in unique_elements:\n",
    "    translations[element] = GoogleTranslator(source='auto', target='en').translate(element)\n",
    "\n",
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.selected_word = self.__get_dictionary(\"data/list_temp_lemmed (1).csv\", sep=\" \")\n",
    "        self.selected_word2 = self.__get_dictionary(\"data/list_temp_lemmed.csv\", sep=\"_\")\n",
    "        \n",
    "        self.__init_synonym_dict()\n",
    "                \n",
    "    def __init_synonym_dict(self):\n",
    "        \"\"\"Membaca file list_temp_lemmed yang merupakan list sinonim\n",
    "        \"\"\"\n",
    "        synonym_dict = {}\n",
    "        for line in open(\"data/list_temp_lemmed.txt\", \"r\"):\n",
    "            words = line.strip().split(\",\")    \n",
    "            for i in range(1, len(words)):\n",
    "                synonym_dict[words[i]] = words[0]\n",
    "                \n",
    "        self.synonym_dict = synonym_dict\n",
    "        \n",
    "    def __get_dictionary(self, file: str, sep : str) -> list:\n",
    "        \"\"\"Untuk membaca file list_temp_lemmed.csv \n",
    "        dan list_temp_lemmed (1).txt\n",
    "\n",
    "        Args:\n",
    "            file (str): path filenya\n",
    "            sep (str): pemisah/separator\n",
    "\n",
    "        Returns:\n",
    "            list: Isi file dalam bentuk list/array\n",
    "        \"\"\"\n",
    "        syn1 = pd.read_csv(\n",
    "            file, header=None, sep=sep,\n",
    "            error_bad_lines=False\n",
    "        )\n",
    "        # Idk now what's going on here\n",
    "        sin_direct2 = syn1.apply(\n",
    "            lambda x: ','.join(x.dropna().astype(str)), \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        for n, line in enumerate(sin_direct2):\n",
    "            sin_direct2[n] = \" \" + line.rstrip() if line.startswith(\"line\") else line.rstrip() \n",
    "        return ','.join(sin_direct2).split(',')\n",
    "        \n",
    "    def pre_process_text(self, sentence: str) -> list:\n",
    "        \"\"\"Melakukan preprocessing kalimat, dimulai dari case folding\n",
    "        menghapus angka, dan menghapus tanda baca kecuali koma, karena\n",
    "        diasumsikan bahwa setiap gejala akan dipisahkan oleh koma\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Satu kalimat yang akan dipreprocessing\n",
    "\n",
    "        Returns:\n",
    "            str: hasil preprocessing dalam bentuk word token\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower() # case folding\n",
    "        sentence = re.sub(\"\\d+\", \"\", sentence) # remove numbers\n",
    "        sentence = sentence.split(',') \n",
    "        sentence = [re.sub(r'[^\\w\\s]', '', symptom) for symptom in sentence] # remove punctuation\n",
    "        return sentence\n",
    "    \n",
    "    def remove_stopword(self, sentence: str) -> str:\n",
    "        \"\"\"Menghapus stopword dalam bahasa inggris\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Satu kalimat \n",
    "\n",
    "        Returns:\n",
    "            str : Kalimat yang sudah dihapus stopwordnya\n",
    "        \"\"\"\n",
    "        if sentence is None: return None\n",
    "        \n",
    "        word_token = nltk.word_tokenize(sentence)\n",
    "        word_token = [word for word in word_token if not word in self.stopwords]\n",
    "        \n",
    "        return \" \".join(word_token) if len(word_token) > 0 else None\n",
    "    \n",
    "    def get_selected_word(self, sentence : str, selected_word : list) -> str:\n",
    "        \"\"\"Menghapus kata-kata pada sentence yang tidak ada pada\n",
    "        list/array `selected_word`\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Kalimat yang akan dihapus kata-katanya\n",
    "            selected_word (list): list kata-kata yang akan dipilih\n",
    "\n",
    "        Returns:\n",
    "            str: _description_\n",
    "        \"\"\"\n",
    "        if sentence is None: return None\n",
    "        \n",
    "        word_token = nltk.word_tokenize(sentence)\n",
    "        word_token = [word for word in word_token if word in selected_word]\n",
    "\n",
    "        return \" \".join(word_token) if len(word_token) > 0 else None\n",
    "    \n",
    "    def remove_none_in_list(self, list : list) -> list:\n",
    "        \"\"\"Menghapus None yang ada pada array/list\n",
    "        \"\"\"\n",
    "        return [word for word in list if word]\n",
    "    \n",
    "    def pos_tagger(self, nltk_tag, noun_to_verb=False):\n",
    "        \"\"\"Mengidentifikasi apakah kata tersebut\n",
    "        verb, noun, atau adv\n",
    "            \n",
    "        Jika `noun_to_verb` bernilai True maka kata dalam bentuk \n",
    "        noun akan dikembalikan kedalam bentuk verb\n",
    "        \"\"\"\n",
    "        \n",
    "        if   nltk_tag.startswith('J'): return wordnet.VERB\n",
    "        elif nltk_tag.startswith('V'): return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'): return wordnet.VERB if noun_to_verb else wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'): return wordnet.ADV\n",
    "        return None\n",
    "    \n",
    "    def tag_sentence(self, sentence : str, noun_to_verb=False) -> str:\n",
    "        \"\"\"Melakukan word tag menggunakan Wordnet pada satu kalimat\n",
    "\n",
    "        Args:\n",
    "            sentence (str): kalimat yang akan dilakukan tag\n",
    "            noun_to_verb (bool, optional): Jika True maka kata noun akan diubah ke verb\n",
    "\n",
    "        Returns:\n",
    "            str: Kalimat hasil tag\n",
    "        \"\"\"\n",
    "        words = []\n",
    "\n",
    "        for word, tag in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
    "            tag = self.pos_tagger(tag, noun_to_verb)\n",
    "            word = self.lemmatizer.lemmatize(word, tag) if tag is not None else word    \n",
    "            words.append(word)\n",
    "            \n",
    "        return \" \".join(words)\n",
    "    \n",
    "    def tag_sentences(self, sentences: list, noun_to_verb=False) -> list:\n",
    "        \"\"\"Melakukan word tag menggunakan Wordnet pada satu banyak kalimat sekaligus\n",
    "\n",
    "        Args:\n",
    "            sentences (list): List kalimat yang akan dilakukan tag\n",
    "            noun_to_verb (bool, optional): Jika True maka kata noun akan diubah ke verb\n",
    "\n",
    "        Returns:\n",
    "            list: List kalimat hasil tag\n",
    "        \"\"\"\n",
    "        return [self.tag_sentence(sentence, noun_to_verb) for sentence in sentences]\n",
    "\n",
    "    def __jieba_split(self, sentence: str) -> list:\n",
    "        \"\"\"Kurang ngerti juga ini untuk apa\n",
    "        \"\"\"\n",
    "        # Without utf-8 encoding, it can't correspond to the words in the tongyici file.\n",
    "        seg_list = jieba.cut(sentence, cut_all = False)\n",
    "        return \"/\".join(seg_list).split(\"/\")\n",
    "    \n",
    "    def tihuan_tongyici(self, sentence, with_jieba=False) -> str:\n",
    "        \"\"\"Ini kenapa namanya tihuan_tongyici?\n",
    "        \"\"\"\n",
    "        word_split = self.__jieba_split(sentence) if with_jieba else sentence.split(\", \")\n",
    "        \n",
    "        final_sentence = \"\"\n",
    "        for word in word_split:\n",
    "            final_sentence += self.synonym_dict[word] if word in self.synonym_dict else word\n",
    "        return final_sentence\n",
    "    \n",
    "    def translate_sentence(self, sentence : str) -> str:\n",
    "        \"\"\"Translate kalimat ke bahasa Inggris\n",
    "        \"\"\"\n",
    "        return GoogleTranslator(source='auto', target='en').translate(sentence)\n",
    "    \n",
    "    def translate_sentences(self, sentences: list) -> list:\n",
    "        \"\"\"Translate list kalimat ke bahasa inggris\n",
    "        \"\"\"\n",
    "        return [self.translate_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    # 4 method dibawah bersifat private dan dibuat agar program lebih \n",
    "    # rapi. Method-method ini hanya digunakan di method transform\n",
    "    # Isi variabel sentances untuk 4 method dibawah adalah sebagai berikut\n",
    "    # [\n",
    "    #    ['stress', 'alcohol']       -> symptomps dari 1 orang\n",
    "    #    ['symptom', 'symptom']      -> symptomps dari 1 orang\n",
    "    # ]\n",
    "    def __tag_sentence(self, sentences: list) -> list:\n",
    "        word_tagged = [self.tag_sentences(symptoms, noun_to_verb=False) for symptoms in sentences]              \n",
    "        return [self.tag_sentences(symptoms, noun_to_verb=True) for symptoms in word_tagged]\n",
    "    \n",
    "    def __remove_stopword(self, sentences : list) -> list:\n",
    "        temp = []\n",
    "        for symptoms in sentences:\n",
    "            temp.append([self.remove_stopword(symptom) for symptom in symptoms])\n",
    "        return temp\n",
    "    \n",
    "    def __get_selected_word(self, sentences: list, selected_word: list) -> list:\n",
    "        temp = []\n",
    "        for symptoms in sentences:\n",
    "            symptoms = [self.get_selected_word(symptom, selected_word) for symptom in symptoms]\n",
    "            temp.append(self.remove_none_in_list(symptoms))\n",
    "        return temp\n",
    "    \n",
    "    def __tihuan_tongyici(self, sentences: list, with_jieba=False):\n",
    "        temp= []\n",
    "        for symptoms in sentences:    \n",
    "            temp.append([self.tihuan_tongyici(symptom, with_jieba) for symptom in symptoms])\n",
    "        return temp\n",
    "            \n",
    "    def transform(self, sentences):\n",
    "        \"\"\"Method utama yang berfungsi untuk melakukan seluruh rangkaian\n",
    "        preprocessing pada text gejala/symptoms\n",
    "        \n",
    "        Args:\n",
    "            sentences (list or np.ndarray or str): List kalimat/kalimat yang akan dilakukan preprocessing.\n",
    "                                                    Bisa diisikan satu kalimat atau banyak.\n",
    "                                                    \n",
    "        Returns:\n",
    "            list or str: hasil preprocessing\n",
    "        \"\"\"\n",
    "        is_array = isinstance(sentences, np.ndarray) or isinstance(sentences, list)\n",
    "        \n",
    "        if not is_array:\n",
    "            if not isinstance(sentences, str):\n",
    "                raise Exception(\"Sentence harus berupa list, np.ndarray, atau string\")\n",
    "            sentences = [sentences]\n",
    "                    \n",
    "        sentences = self.translate_sentences(sentences)\n",
    "        sentences = [self.pre_process_text(symptoms) for symptoms in sentences]\n",
    "        \n",
    "        # mulai dari sini symptoms telah dipisah dalam bentuk array. Sehingga isi\n",
    "        # dari variabel sentances seperti berikut ini\n",
    "        # [\n",
    "        #    ['stress', 'alcohol']       -> symptomps dari 1 orang\n",
    "        #    ['symptom', 'symptom']      -> symptomps dari 1 orang\n",
    "        # ]\n",
    "        sentences = self.__tag_sentence(sentences)\n",
    "        sentences = self.__remove_stopword(sentences)\n",
    "        sentences = self.__get_selected_word(sentences, self.selected_word)\n",
    "        sentences = self.__tihuan_tongyici(sentences)\n",
    "        sentences = self.__tihuan_tongyici(sentences, with_jieba=True)\n",
    "        sentences = self.__get_selected_word(sentences, self.selected_word2)\n",
    "        \n",
    "        # join\n",
    "        sentences = [\" \".join(symptoms) for symptoms in sentences]\n",
    "        \n",
    "        return sentences if is_array else sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8: expected 46 fields, saw 56\\n'\n",
      "b'Skipping line 8: expected 25 fields, saw 29\\n'\n"
     ]
    }
   ],
   "source": [
    "processing = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ASUS\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.860 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['worry',\n",
       " 'concentrate alcohol',\n",
       " 'anxious',\n",
       " 'dizzy',\n",
       " 'faint',\n",
       " 'mood sad',\n",
       " 'sweat tremble panic',\n",
       " 'tremble concentrate',\n",
       " 'alcohol alcohol',\n",
       " 'panic']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = processing.transform(unique_elements)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worry'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.transform('Often feeling worrying too much')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>anxious</th>\n",
       "      <th>concentrate</th>\n",
       "      <th>dizzy</th>\n",
       "      <th>faint</th>\n",
       "      <th>mood</th>\n",
       "      <th>panic</th>\n",
       "      <th>sad</th>\n",
       "      <th>sweat</th>\n",
       "      <th>tremble</th>\n",
       "      <th>worry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  anxious  concentrate  dizzy  faint  mood  panic  sad  sweat  \\\n",
       "0        0        0            0      0      0     0      0    0      0   \n",
       "1        1        0            1      0      0     0      0    0      0   \n",
       "2        0        1            0      0      0     0      0    0      0   \n",
       "3        0        0            0      1      0     0      0    0      0   \n",
       "4        0        0            0      0      1     0      0    0      0   \n",
       "5        0        0            0      0      0     1      0    1      0   \n",
       "6        0        0            0      0      0     0      1    0      1   \n",
       "7        0        0            1      0      0     0      0    0      0   \n",
       "8        1        0            0      0      0     0      0    0      0   \n",
       "9        0        0            0      0      0     0      1    0      0   \n",
       "\n",
       "   tremble  worry  \n",
       "0        0      1  \n",
       "1        0      0  \n",
       "2        0      0  \n",
       "3        0      0  \n",
       "4        0      0  \n",
       "5        0      0  \n",
       "6        1      0  \n",
       "7        1      0  \n",
       "8        0      0  \n",
       "9        0      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=len(result), \n",
    "    min_df=0,\n",
    "    decode_error='ignore',\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(result)\n",
    "pd.DataFrame(tf.toarray(), columns=tf_vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a96b9fde85eae18f328e26a2214f5533e885c6cdd021089c2c5b6c440d89605"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('metamorphosis-modeling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
