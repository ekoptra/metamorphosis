{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_excel(\"D:/META LAB/Excel/symp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deep-translator in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (from deep-translator) (4.10.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (from deep-translator) (2.26.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.0.1 in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (from deep-translator) (8.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (from click<9.0.0,>=8.0.1->deep-translator) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.7)\n",
      "Requirement already satisfied: autocorrect in c:\\users\\zalramli\\anaconda3\\lib\\site-packages (2.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zalramli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\zalramli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zalramli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zalramli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install deep-translator\n",
    "from deep_translator import GoogleTranslator\n",
    "!pip install autocorrect\n",
    "from string import digits\n",
    "import regex as re\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "import jieba\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_PreProcess:\n",
    "\n",
    "  def __init__(self,data):\n",
    "\n",
    "    # Translate\n",
    "    translations=[]\n",
    "    # Deteksi indo, translate ke inggris\n",
    "    translations = {}\n",
    "    for column in data.columns:\n",
    "        # Unique elements of the column\n",
    "        unique_elements = data[column].unique()\n",
    "    for element in unique_elements:\n",
    "        # Adding all the translations to a dictionary (translations)\n",
    "        translations[element] = GoogleTranslator(source='auto', target='en').translate(element)\n",
    "    data.replace(translations, inplace = True)\n",
    "    data=pd.DataFrame(data[\"q_11\"])[\"q_11\"]\n",
    "    \n",
    "    # Case Folding\n",
    "    casefolded=[]\n",
    "    for line in data:\n",
    "      a=line.lower()\n",
    "      casefolded.append(a)\n",
    "    \n",
    "    # Omit Number and Punctuations\n",
    "    number=[]\n",
    "    for line in casefolded:\n",
    "      remove_digits=str.maketrans(' ', ' ', digits)\n",
    "      result=line.translate(remove_digits)\n",
    "      number.append(result)\n",
    "    punct=[]\n",
    "    for char in number:\n",
    "      result=re.sub(r\"\\p{P}(?<!,)\",\"\", char)\n",
    "      punct.append(result)\n",
    "    \n",
    "    # Split by Comma\n",
    "    splitted=[]\n",
    "    for line in punct:\n",
    "      a=line.split(',')\n",
    "      splitted.append(a)\n",
    "    \n",
    "    # Lemmatize 1\n",
    "    def pos_tagger(nltk_tag):\n",
    "      if nltk_tag.startswith('J'):\n",
    "        return wordnet.VERB\n",
    "      elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "      elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "      elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "      else:          \n",
    "        return None\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tagged=[]\n",
    "    for i in range(len(splitted)):\n",
    "      a=[]\n",
    "      for j in range(len(splitted[i])):\n",
    "        b=nltk.pos_tag(nltk.word_tokenize(splitted[i][j]))\n",
    "        a.append(b)\n",
    "      pos_tagged.append(a)\n",
    "    \n",
    "    wordnet_tagged=[]\n",
    "    for i in range(len(pos_tagged)):\n",
    "      a=[]\n",
    "      for j in range(len(pos_tagged[i])):\n",
    "        b=list(map(lambda x: (x[0], pos_tagger(x[1])),pos_tagged[i][j]))\n",
    "        a.append(b)\n",
    "      wordnet_tagged.append(a)\n",
    "    \n",
    "    lem = []\n",
    "    for i in range(len(wordnet_tagged)):\n",
    "      a=[]\n",
    "      for j in range(len(wordnet_tagged[i])):\n",
    "        b=[]\n",
    "        for word, tag in wordnet_tagged[i][j]:\n",
    "          if tag is None:\n",
    "            b.append(word)\n",
    "          else:\n",
    "            b.append(lemmatizer.lemmatize(word, tag))\n",
    "        b=\" \".join(b)\n",
    "        a.append(b)\n",
    "      lem.append(a)\n",
    "\n",
    "    # Lemmatize 2\n",
    "    def pos_tagger(nltk_tag):\n",
    "      if nltk_tag.startswith('J'):\n",
    "        return wordnet.VERB\n",
    "      elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "      elif nltk_tag.startswith('N'):\n",
    "        return wordnet.VERB\n",
    "      elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "      else:          \n",
    "        return None\n",
    "    \n",
    "    pos_tagged=[]\n",
    "    for i in range(len(lem)):\n",
    "      a=[]\n",
    "      for j in range(len(lem[i])):\n",
    "        b=nltk.pos_tag(nltk.word_tokenize(lem[i][j]))\n",
    "        a.append(b)\n",
    "      pos_tagged.append(a)\n",
    "   \n",
    "    wordnet_tagged=[]\n",
    "    for i in range(len(pos_tagged)):\n",
    "      a=[]\n",
    "      for j in range(len(pos_tagged[i])):\n",
    "        b=list(map(lambda x: (x[0], pos_tagger(x[1])),pos_tagged[i][j]))\n",
    "        a.append(b)\n",
    "      wordnet_tagged.append(a)\n",
    "    \n",
    "    lem2 = []\n",
    "    for i in range(len(wordnet_tagged)):\n",
    "      a=[]\n",
    "      for j in range(len(wordnet_tagged[i])):\n",
    "        b=[]\n",
    "        for word, tag in wordnet_tagged[i][j]:\n",
    "          if tag is None:\n",
    "            b.append(word)\n",
    "          else:\n",
    "            b.append(lemmatizer.lemmatize(word, tag))\n",
    "        b=\" \".join(b)\n",
    "        a.append(b)\n",
    "      lem2.append(a)\n",
    "\n",
    "    # Remove Stopwords 1\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    removestop = [] \n",
    "    for i in range(len(lem2)):\n",
    "      a=[]\n",
    "      for j in range(len(lem2[i])):\n",
    "        word_token=nltk.word_tokenize(lem2[i][j])\n",
    "        word_token=[word for word in word_token if not word in stopwords]\n",
    "        a.append(\" \".join(word_token))\n",
    "      removestop.append(a)\n",
    "\n",
    "    syn1 = pd.read_csv(\"D:/META LAB/Drive/list_temp_lemmed (1).csv\",header=None,sep=\" \",error_bad_lines=False)\n",
    "    sin_direct2= syn1[syn1.columns[0:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1)\n",
    "\n",
    "    # Remove Stopwords 2\n",
    "    for n,line in enumerate(sin_direct2):\n",
    "      if line.startswith(\"line\"):\n",
    "        sin_direct2[n] = \" \"+line.rstrip()\n",
    "      else:\n",
    "        sin_direct2[n]=line.rstrip()\n",
    "    sw1=','.join(sin_direct2)\n",
    "    sw1=sw1.split(',')\n",
    "\n",
    "    removestop2 = []\n",
    "    for i in range(len(removestop)):\n",
    "      a=[]\n",
    "      for j in range(len(removestop[i])):\n",
    "        word_token=nltk.word_tokenize(removestop[i][j])\n",
    "        word_token=[word for word in word_token if word in sw1]\n",
    "        a.append(\" \".join(word_token))\n",
    "      removestop2.append(a)\n",
    "\n",
    "# Synonym Replacemnet 1\n",
    "    def tihuan_tongyici(string1):\n",
    "      combine_dict = {}\n",
    "      for line in open(\"D:/META LAB/Drive/list_temp_lemmed.txt\", \"r\"):\n",
    "        seperate_word = line.strip().split(\",\")\n",
    "        num = len(seperate_word)\n",
    "        for i in range(1, num):\n",
    "            combine_dict[seperate_word[i]] = seperate_word[0]    \n",
    "      \n",
    "      f = string1\n",
    "      \n",
    "      final_sentence = \"\"\n",
    "      for word in f.split(\", \"):\n",
    "        if word in combine_dict:\n",
    "          word = combine_dict[word]\n",
    "          final_sentence += word\n",
    "        else:\n",
    "          final_sentence += word\n",
    "      return final_sentence\n",
    "\n",
    "    syno = []\n",
    "    for i in range(len(removestop2)):\n",
    "      a=[]\n",
    "      for j in range(len(removestop2[i])):\n",
    "        word_token=tihuan_tongyici(removestop2[i][j])\n",
    "        a.append(\"\".join(word_token))\n",
    "      syno.append(a)\n",
    "      \n",
    "# Synonym Replacemnet 2\n",
    "    def tihuan_tongyici1(string1):\n",
    "      combine_dict = {}\n",
    "      for line in open(\"D:/META LAB/Drive/list_temp_lemmed.txt\", \"r\"):\n",
    "        seperate_word = line.strip().split(\",\")\n",
    "        num = len(seperate_word)\n",
    "        for i in range(1, num):\n",
    "            combine_dict[seperate_word[i]] = seperate_word[0]    \n",
    "      \n",
    "      seg_list = jieba.cut(string1, cut_all = False)\n",
    "      f = \"/\".join(seg_list) # Without utf-8 encoding, it can't correspond to the words in the tongyici file.\n",
    "      final_sentence = \"\"\n",
    "      for word in f.split(\"/\"):\n",
    "        if word in combine_dict:\n",
    "            word = combine_dict[word]\n",
    "            final_sentence += word\n",
    "        else:\n",
    "            final_sentence += word\n",
    "    # print final_sentence\n",
    "      return final_sentence\n",
    "\n",
    "    syno1 = []\n",
    "    for i in range(len(syno)):\n",
    "      a=[]\n",
    "      for j in range(len(syno[i])):\n",
    "        word_token=tihuan_tongyici1(syno[i][j])\n",
    "        a.append(\"\".join(word_token))\n",
    "      syno1.append(a)\n",
    "\n",
    "    syn = pd.read_csv(\"D:/META LAB/Drive/list_temp_lemmed.csv\",header=None,sep=\"_\",error_bad_lines=False)\n",
    "    sin_direct= syn[syn.columns[0:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1)\n",
    "          \n",
    "#remove stopwords 3\n",
    "    for n,line in enumerate(sin_direct):\n",
    "      if line.startswith(\"line\"):\n",
    "        sin_direct[n] = \" \"+line.rstrip()\n",
    "      else:\n",
    "        sin_direct[n]=line.rstrip()\n",
    "    sw2=','.join(sin_direct)\n",
    "    sw2=sw2.split(',')\n",
    "\n",
    "    removestop3 = []\n",
    "    for i in range(len(syno1)):\n",
    "      a=[]\n",
    "      for j in range(len(syno1[i])):\n",
    "        word_token=nltk.word_tokenize(syno1[i][j])\n",
    "        word_token=[word for word in word_token if word in sw2]\n",
    "        a.append(\" \".join(word_token))\n",
    "      removestop3.append(a)\n",
    "    \n",
    "    # Join\n",
    "    joined=[]\n",
    "    for line in removestop3:\n",
    "      a=\" \".join(line)\n",
    "      joined.append(a)\n",
    "    \n",
    "    # Vectorize\n",
    "    tf_vectorizer = CountVectorizer(max_df=len(joined), \n",
    "                                     min_df=0,\n",
    "                                     decode_error='ignore',binary=True)\n",
    "    tf = tf_vectorizer.fit_transform(joined)\n",
    "    self.symptoms = pd.DataFrame(tf.toarray(), columns=tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zalramli\\AppData\\Local\\Temp/ipykernel_11980/1234393015.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  x=M_PreProcess(data)\n",
      "b'Skipping line 8: expected 46 fields, saw 56\\n'\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\zalramli\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.449 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "b'Skipping line 8: expected 25 fields, saw 29\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addict</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>anger</th>\n",
       "      <th>anxious</th>\n",
       "      <th>appetite</th>\n",
       "      <th>balance</th>\n",
       "      <th>breathe</th>\n",
       "      <th>communicate</th>\n",
       "      <th>concentrate</th>\n",
       "      <th>confuse</th>\n",
       "      <th>...</th>\n",
       "      <th>suicide</th>\n",
       "      <th>sweat</th>\n",
       "      <th>tire</th>\n",
       "      <th>trauma</th>\n",
       "      <th>tremble</th>\n",
       "      <th>unconscious</th>\n",
       "      <th>violence</th>\n",
       "      <th>weight</th>\n",
       "      <th>withdrawal</th>\n",
       "      <th>worry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     addict  alcohol  anger  anxious  appetite  balance  breathe  communicate  \\\n",
       "0         0        0      0        0         0        0        0            0   \n",
       "1         0        1      0        0         0        0        0            0   \n",
       "2         0        0      0        1         0        0        0            0   \n",
       "3         0        0      0        0         0        0        0            0   \n",
       "4         0        0      0        0         0        0        0            0   \n",
       "..      ...      ...    ...      ...       ...      ...      ...          ...   \n",
       "151       0        0      0        0         0        0        0            0   \n",
       "152       0        0      0        0         0        0        0            0   \n",
       "153       0        0      1        0         0        0        0            0   \n",
       "154       0        0      0        1         0        0        0            0   \n",
       "155       0        0      1        1         0        0        1            0   \n",
       "\n",
       "     concentrate  confuse  ...  suicide  sweat  tire  trauma  tremble  \\\n",
       "0              0        0  ...        0      0     0       0        0   \n",
       "1              1        0  ...        0      0     0       0        0   \n",
       "2              0        0  ...        0      0     0       0        0   \n",
       "3              0        0  ...        1      0     0       0        0   \n",
       "4              0        0  ...        0      0     0       0        0   \n",
       "..           ...      ...  ...      ...    ...   ...     ...      ...   \n",
       "151            0        0  ...        0      0     0       0        0   \n",
       "152            0        0  ...        1      0     0       0        0   \n",
       "153            0        0  ...        1      0     0       0        0   \n",
       "154            1        0  ...        1      0     1       0        0   \n",
       "155            0        0  ...        1      0     0       0        1   \n",
       "\n",
       "     unconscious  violence  weight  withdrawal  worry  \n",
       "0              0         0       0           0      1  \n",
       "1              0         0       0           0      0  \n",
       "2              0         0       0           0      0  \n",
       "3              0         0       0           0      0  \n",
       "4              0         0       0           0      0  \n",
       "..           ...       ...     ...         ...    ...  \n",
       "151            0         0       0           0      0  \n",
       "152            0         0       0           0      0  \n",
       "153            0         0       0           0      0  \n",
       "154            0         0       0           0      0  \n",
       "155            0         0       0           0      0  \n",
       "\n",
       "[156 rows x 59 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=M_PreProcess(data)\n",
    "x.symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.symptoms.to_excel (\"D:/META LAB/Excel/symp_fix.xlsx\", index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
